{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "#搭建生成网络(反卷积)，输入100，输出28x28\n",
    "def generator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=100, units=1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization()) #使得其输出数据的均值接近0，其标准差接近1;防止梯度消失和过拟合\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Reshape((7, 7, 128), input_shape=(128*7*7,))) #将输入shape转换为特定的shape\n",
    "    model.add(UpSampling2D(size=(2, 2))) #二维上采样，将数据的行和列分别重复size[0]和size[1]次(扩大矩阵，可以用于复原图像)\n",
    "    model.add(Conv2D(64, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(UpSampling2D(size=(2, 2)))\n",
    "    model.add(Conv2D(1, (5, 5), padding='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "#搭建判别网络，输入28x28，输出1\n",
    "def discriminator_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (5, 5), padding='same', input_shape=(28, 28, 1)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (5, 5)))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "# 联合生成和判别两个网络\n",
    "def generator_containing_discriminator(g, d):\n",
    "    model = Sequential()\n",
    "    model.add(g)\n",
    "    d.trainable = False\n",
    "    model.add(d)\n",
    "    return model\n",
    "\n",
    "# 结合所有生成网络预测生成的图片：一个batch共生成128张图片进行图片拼接\n",
    "def combine_images(generated_images):\n",
    "    num = generated_images.shape[0]\n",
    "    width = int(math.sqrt(num)) #11\n",
    "    height = int(math.ceil(float(num)/width)) #12\n",
    "    shape = generated_images.shape[1:3]\n",
    "    image = np.zeros((height*shape[0], width*shape[1]),dtype=generated_images.dtype)\n",
    "    for index, img in enumerate(generated_images):\n",
    "        i = int(index/width)\n",
    "        j = index % width\n",
    "        image[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = img[:, :, 0] #28*28*1\n",
    "    return image\n",
    "\n",
    "#模型训练\n",
    "def train(BATCH_SIZE):\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "    X_train = X_train[:, :, :, None]\n",
    "    X_test = X_test[:, :, :, None]\n",
    "    # X_train = X_train.reshape((X_train.shape, 1) + X_train.shape[1:])\n",
    "    d = discriminator_model()\n",
    "    g = generator_model()\n",
    "    d_on_g = generator_containing_discriminator(g, d)\n",
    "    d_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g_optim = SGD(lr=0.0005, momentum=0.9, nesterov=True)\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    d_on_g.compile(loss='binary_crossentropy', optimizer=g_optim)\n",
    "    d.trainable = True\n",
    "    d.compile(loss='binary_crossentropy', optimizer=d_optim)\n",
    "    \n",
    "    for epoch in range(100):\n",
    "        print(\"Epoch is\", epoch)\n",
    "        print(\"Number of batches\", int(X_train.shape[0]/BATCH_SIZE))\n",
    "        for index in range(int(X_train.shape[0]/BATCH_SIZE)):\n",
    "            noise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100)) #产生大小为BATCH_SIZE*100，值在[-1,1)之间的均匀分布\n",
    "            image_batch = X_train[index*BATCH_SIZE : (index+1)*BATCH_SIZE]\n",
    "            generated_images = g.predict(noise, verbose=0)\n",
    "            if index % 20 == 0:\n",
    "                image = combine_images(generated_images)\n",
    "                image = image*127.5+127.5\n",
    "                Image.fromarray(image.astype(np.uint8)).save(str(epoch)+\"_\"+str(index)+\".png\")\n",
    "            X = np.concatenate((image_batch, generated_images)) #数组拼接（上下）\n",
    "            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE #[1,1,1,...,1,0,0,0,...,0]\n",
    "            d_loss = d.train_on_batch(X, y)\n",
    "            print(\"batch %d d_loss : %f\" % (index, d_loss))\n",
    "            noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "            d.trainable = False\n",
    "            g_loss = d_on_g.train_on_batch(noise, [1] * BATCH_SIZE)\n",
    "            d.trainable = True\n",
    "            print(\"batch %d g_loss : %f\" % (index, g_loss))\n",
    "            if index % 10 == 9:\n",
    "                g.save_weights('generator', True)\n",
    "                d.save_weights('discriminator', True)\n",
    "\n",
    "#生成预测\n",
    "def generate(BATCH_SIZE, nice=False):\n",
    "    g = generator_model()\n",
    "    g.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "    g.load_weights('generator')\n",
    "    if nice:\n",
    "        d = discriminator_model()\n",
    "        d.compile(loss='binary_crossentropy', optimizer=\"SGD\")\n",
    "        d.load_weights('discriminator')\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE*20, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        d_pret = d.predict(generated_images, verbose=1)\n",
    "        index = np.arange(0, BATCH_SIZE*20)\n",
    "        index.resize((BATCH_SIZE*20, 1))\n",
    "        pre_with_index = list(np.append(d_pret, index, axis=1))\n",
    "        pre_with_index.sort(key=lambda x: x[0], reverse=True) #依第一列值大小降序排列\n",
    "        nice_images = np.zeros((BATCH_SIZE,) + generated_images.shape[1:3], dtype=np.float32)\n",
    "        nice_images = nice_images[:, :, :, None]\n",
    "        for i in range(BATCH_SIZE):\n",
    "            idx = int(pre_with_index[i][1])\n",
    "            nice_images[i, :, :, 0] = generated_images[idx, :, :, 0]\n",
    "        image = combine_images(nice_images)\n",
    "    else:\n",
    "        noise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "        generated_images = g.predict(noise, verbose=1)\n",
    "        image = combine_images(generated_images)\n",
    "    image = image*127.5+127.5\n",
    "    Image.fromarray(image.astype(np.uint8)).save(\"generated_image.png\")\n",
    "\n",
    "#终端获取参数是训练还是生成预测\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--mode\", type=str)\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=128)\n",
    "    parser.add_argument(\"--nice\", dest=\"nice\", action=\"store_true\")\n",
    "    parser.set_defaults(nice=False)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "'''\n",
    "训练：\n",
    "终端输入： python dcgan.py --mode train\n",
    "生成：\n",
    "终端输入： python dcgan.py --mode generate\n",
    "'''\n",
    "if __name__ == \"__main__\":\n",
    "    args = get_args()\n",
    "    if args.mode == \"train\":\n",
    "        train(BATCH_SIZE=args.batch_size)\n",
    "    elif args.mode == \"generate\":\n",
    "        generate(BATCH_SIZE=args.batch_size, nice=args.nice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[ 1  4]\n",
      " [ 7 10]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1, 4, 2], [11, 14, 12], [4, 10, 19], [6, 10, 9]] (4, 3)\n",
      "[[11, 14, 12], [6, 10, 9], [4, 10, 19], [1, 4, 2]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[[1,2,3],\n",
    "               [4,5,6]],\n",
    "              [[7,8,9],\n",
    "               [10,11,12]]])\n",
    "print(x.shape)\n",
    "# x = np.transpose(x,(2,1,0))\n",
    "# print(x.shape)\n",
    "print(x[:, :, 0])\n",
    "\n",
    "y = [1] * 10 + [0] * 10\n",
    "print(y)\n",
    "\n",
    "ls = list([[1,4,2],[11,14,12],[4,10,19],[6,10,9]])\n",
    "print(ls,np.shape(ls))\n",
    "ls.sort(key=lambda x : x[0],reverse=True)\n",
    "print(ls)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
